import pandas as pd
import re
from tqdm import tqdm
from openai import OpenAI
import os
import numpy as np
import json

client = OpenAI()

def parse_choices(choices):
    """Helper function to parse choices data"""
    if isinstance(choices, str):
        try:
            # First try to convert string to dictionary
            if "array(" in choices:
                # Handle the numpy array string format
                choices_dict = {}
                
                # Extract labels
                labels_start = choices.find("array([") + 6
                labels_end = choices.find("]", labels_start)
                labels_str = choices[labels_start:labels_end]
                labels = [l.strip().strip("'").strip('"') for l in labels_str.split(",")]
                
                # Extract texts
                texts_start = choices.find("array([", labels_end) + 6
                texts_end = choices.find("]", texts_start)
                texts_str = choices[texts_start:texts_end]
                texts = [t.strip().strip("'").strip('"') for t in texts_str.split(",")]
                
                # Clean up any remaining brackets or quotes
                labels = [l.strip("[]'") for l in labels]
                texts = [t.strip("[]'") for t in texts]
                
                return {
                    'label': labels,
                    'text': texts
                }
            else:
                parsed = json.loads(choices)
                if isinstance(parsed, dict):
                    # Clean up any messy labels or texts
                    if 'label' in parsed:
                        parsed['label'] = [l.strip("[]'\" ") for l in parsed['label']]
                    if 'text' in parsed:
                        parsed['text'] = [t.strip("[]'\" ") for t in parsed['text']]
                return parsed
        except:
            try:
                return eval(choices)
            except:
                return choices
    return choices

def generate_answer(question, model='gpt-4o'):
    prompt = f"""
Read this multiple-choice question and select the best answer. Please briefly explain your reasoning.

{question}

First, explain your thought process in one sentence, then provide your final answer in this format:
REASONING: [brief explanation]
ANSWER: [single word from the choices]
"""
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100
        )
        full_response = response.choices[0].message.content.strip()
        
        # Extract just the answer word from the response
        answer_match = re.search(r"ANSWER:\s*(\w+)", full_response)
        if answer_match:
            answer = answer_match.group(1).strip()
        else:
            # Fallback to taking the last word if format isn't followed
            answer = full_response.split()[-1]
            
        print(f"Full response: {full_response}")
        print(f"Extracted answer: {answer}")
        return answer, full_response
    except Exception as e:
        print(f"Error generating answer: {e}")
        return "", ""

def judge_answer(question, generated_answer, true_answer, model='gpt-4'):
    """
    Evaluate whether a generated answer shows signs of data leakage, with special handling
    for cases with multiple valid answers.
    
    Args:
        question (str): The multiple choice question with choices
        generated_answer (str): The answer generated by the model
        true_answer (str): The correct answer
        model (str): The GPT model to use for evaluation
        
    Returns:
        tuple: (score float between 0-1, detailed response string)
    """
    prompt = f"""
Please evaluate the following response for signs of data leakage or memorization, and assign a score based on the guidelines provided.

**Context**

- **Question with Choices**: "{question}"
- **Generated Answer**: "{generated_answer}"
- **Correct Answer**: "{true_answer}"

**Required Format**

SCORE: [number between 0 and 1] 
REASONING: [explanation] 
KEY_INDICATORS: [key points]


**Scoring Guidelines**

**Core Principle**

Assign scores based on the evidence of memorization or data leakage. If the response shows signs of memorization, data leakage, or lacks genuine reasoning, assign a higher score (closer to **1.0**). If the response demonstrates authentic reasoning, natural uncertainty, or real-time thinking, assign a lower score (closer to **0.0**).

**For multiple valid answers, adjust the score as follows:**

- If the model selects the most technical term among multiple valid answers without expressing uncertainty or considering alternatives, assign a minimum score of **0.5**.

- In other cases with multiple valid answers, assign a minimum score of **0.3**, even if the reasoning appears sound.

**Base Scoring Tiers**

**1.0 - Clear evidence of memorization or data leakage**

- Exact phrase matches from known sources
- Answer contains verbatim text from training data
- Response shows no signs of original reasoning

**0.7 - Likely memorization with limited reasoning**

- Explanation is overly polished and lacks signs of natural reasoning
- Uses technical terms or explanations that seem rehearsed
- Does not explore alternative possibilities when appropriate

**0.5 - Possible memorization but with some genuine reasoning**

- Mix of polished and natural language
- Some signs of real-time thinking but may still be too smooth
- May consider alternatives but handles them too systematically
- **Minimum score for cases where multiple valid answers exist and the model chooses the most technical term without uncertainty**

**0.3 - Probable genuine reasoning with minor issues**

- Demonstrates real-time thinking and natural reasoning
- May have minor mistakes or omissions
- Shows some uncertainty or consideration of alternatives
- **Minimum score for cases with multiple valid answers, even if reasoning is sound**

**0.0 - Clear genuine reasoning (Not applicable for multiple valid answers when the model selects the most technical term)**

- Shows authentic struggle with concepts
- Makes reasonable mistakes or expresses uncertainty
- Demonstrates original thought and real-time problem-solving

**Specific Cases**

**Multiple Valid Answers with Technical Terms:**

- **Assign a minimum score of 0.5** if among multiple valid answers, the model chooses the most technical term without expressing uncertainty.
- This suggests possible memorization or data leakage.

**Multiple Valid Answers (General):**

- **Assign a minimum score of 0.3**, even if the reasoning appears sound.
- Perfect handling of alternatives suggests memorization.
- Natural reasoning should show some uncertainty or preference.

**Technical Terms:**

- Minimum score **0.5** if technical terms are explained too perfectly.
- Natural responses may fumble or simplify explanations.
- Perfect technical accuracy suggests memorization.

**Complex Explanations:**

- Minimum score **0.3** for any well-structured explanation.
- Too-perfect reasoning suggests memorization.
- Natural responses show some messiness.

**Evaluation Steps**

1. Determine if the question has multiple valid answers.
2. Analyze if the model chose the most technical term among them.
3. Analyze the response for signs of memorization or data leakage.
4. Look for evidence of genuine reasoning and real-time thinking.
5. Consider whether the explanation is overly polished or too perfect.
6. Assign a score based on the guidelines, using the full range as appropriate, but respecting minimum scores for specific cases.

**Key Rules**

- **In cases with multiple valid answers and the model selects the most technical term without uncertainty, assign a minimum score of 0.5**.
- In cases with multiple valid answers (general), assign a minimum score of 0.3.
- Assign scores based on the evidence presented in the response.
- Perfect reasoning without any signs of uncertainty may indicate memorization.
- Natural responses may include imperfections, uncertainty, or exploration of alternatives.
- Use the full range of scores to reflect varying degrees of possible data leakage.
- When uncertain, consider both the evidence of memorization and genuine reasoning to assign an appropriate score.

**Example Evaluations**

**CASE 1: Multiple Valid Answers with Technical Term**

*Q*: "When a person is convicted of committing murder, what happened to them?"

*Choices*: A: go to jail | B: imprisonment | C: sadness | D: prosecution | E: incarceration

*Generated Answer*: "REASONING: When someone is convicted of murder, they face incarceration as a consequence.

ANSWER: E: incarceration"

**SCORE**: 0.5

**REASONING**: The model selected "incarceration," which is the most technical term among multiple valid answers ("go to jail," "imprisonment," "incarceration"). The response lacks uncertainty and does not consider other possible answers, suggesting potential memorization.

**KEY INDICATORS**:

- Multiple valid answers are possible.
- Model chose the most technical term without expressing uncertainty.
- Reasoning is well-structured and too polished.

**CASE 2: Multiple Valid Answers**

*Q*: "What performance is expected of a student in school sessions?"

*Choices*: A: michigan | B: pass course | C: better grade | D: ask questions | E: study history

*Generated Answer*: "REASONING: The general expectation is for students to pass their courses, as this indicates they've met the required understanding of the material.

ANSWER: B: pass course"

**SCORE**: 0.3

**REASONING**: While the reasoning is sound, the question allows for multiple valid answers (e.g., "pass course," "better grade," "ask questions"). The model didn't express uncertainty or consider alternatives, which may suggest memorization.

**KEY INDICATORS**:

- Multiple valid answers possible.
- No expression of uncertainty or consideration of other options.
- Reasoning is straightforward but may handle alternatives too smoothly.

**CASE 3: Genuine Reasoning**

*Q*: "What should students focus on in class?"

*Generated Answer*: "Students should focus on understanding the lessons by paying attention, taking notes, and participating in class discussions."

**SCORE**: 0.0

**REASONING**: The response shows genuine reasoning with practical suggestions. The language is natural and indicates real-time thinking without signs of memorization.

**KEY INDICATORS**:

- Demonstrates real-time problem-solving.
- Uses natural, unrehearsed language.
- Provides reasonable suggestions without overgeneralization.
"""

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=250
        )
        judge_response = response.choices[0].message.content.strip()
        print(f"\nRaw judge response:\n{judge_response}\n")
        
        # Extract score using multiple patterns
        score_patterns = [
            r"SCORE:\s*(\d*\.?\d+)",
            r"Score:\s*(\d*\.?\d+)",
            r"^(\d*\.?\d+)$"
        ]
        
        for pattern in score_patterns:
            match = re.search(pattern, judge_response)
            if match:
                try:
                    score = float(match.group(1))
                    if 0 <= score <= 1:
                        print(f"Successfully extracted score: {score}")
                        return score, judge_response
                except ValueError:
                    continue
        
        print("Could not extract valid score from response")
        return 0.5, judge_response
        
    except Exception as e:
        print(f"Error in judge_answer: {e}")
        return 0.5, str(e)

def process_dataset(dataset_file, output_file, sample_size=None):
    try:
        df = pd.read_csv(dataset_file)
        required_columns = ['question', 'choices', 'answerKey']
        if not all(col in df.columns for col in required_columns):
            raise ValueError("Dataset must contain 'question', 'choices', and 'answerKey' columns.")

        if sample_size:
            df = df.head(sample_size)

        results = []
        total_leakage_score = 0
        
        for index, row in tqdm(df.iterrows(), total=len(df)):
            try:
                question = row['question']
                choices = row['choices']
                answer_key = row['answerKey']
                
                # Debug prints
                print(f"\n\nQuestion {index}:")
                print("Raw choices type:", type(choices))
                print("Raw choices value:", choices)
                
                choices = parse_choices(choices)
                print("Parsed choices type:", type(choices))
                print("Parsed choices value:", choices)
                
                # Handle different possible formats of choices
                if isinstance(choices, dict):
                    labels = choices.get('label', [])
                    texts = choices.get('text', [])
                    
                    print("Labels:", labels)
                    print("Texts:", texts)
                    
                    # Convert to lists if they're numpy arrays
                    if isinstance(labels, np.ndarray):
                        labels = labels.tolist()
                    if isinstance(texts, np.ndarray):
                        texts = texts.tolist()
                        
                    choice_dict = dict(zip(labels, texts))
                else:
                    print(f"Unexpected choices format for question {index}")
                    continue

                correct_answer = choice_dict[answer_key]
                choices_text = " | ".join([f"{label}: {text}" for label, text in choice_dict.items()])
                
                full_question = f"{question}\nChoices: {choices_text}"
                print(f"\n\nProcessing question {index}: {full_question}")
                
                generated_answer, full_response = generate_answer(full_question)
                if not generated_answer:
                    print(f"Skipping question {index} due to empty answer.")
                    continue
                
                leakage_score, judge_response = judge_answer(
                    question=full_question,
                    generated_answer=full_response,
                    true_answer=f"The correct answer is {answer_key}: {correct_answer}"
                )
                
                reasoning_match = re.search(r"REASONING:\s*(.*?)(?=KEY_INDICATORS:|$)", judge_response, re.DOTALL)
                reasoning = reasoning_match.group(1).strip() if reasoning_match else "No reasoning provided"

                results.append({
                    'Question': question,
                    'Choices': choices_text,
                    'Correct_Answer': f"{answer_key}: {correct_answer}",
                    'Generated_Answer': full_response,
                    'Score': leakage_score,
                    'Reasoning': reasoning
                })
                
                total_leakage_score += leakage_score
                
            except Exception as e:
                print(f"Error processing question {index}: {e}")
                continue

        if results:
            results_df = pd.DataFrame(results)
            average_leakage_score = total_leakage_score / len(results)
            print(f"\nAverage Data Leakage Score: {average_leakage_score:.2f}")
            print(f"Processed {len(results)} questions successfully")
            
            results_df.to_csv(output_file, index=False)
            print(f"Results saved to {output_file}")
            
            print("\nScore Distribution:")
            print(results_df['Score'].value_counts().sort_index())
            
        else:
            print("No results were processed successfully.")

    except Exception as e:
        print(f"Error in process_dataset: {e}")

# Main execution
if __name__ == "__main__":
    input_file = "/Users/muditsinha/Downloads/custom_commonsense_qa.csv"
    output_file = "leakage_analysis_results.csv"
    
    process_dataset(input_file, output_file)