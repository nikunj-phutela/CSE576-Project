{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b513dcb9-49c3-4959-9fe2-0cb524333dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import random\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "safe_globals = {\"__builtins__\": {}}\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64c660f-c12d-456a-9348-175ddb5d85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee86984-b9af-4bee-9244-b85673683269",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../Data/commonsense_qa.csv\")\n",
    "result = {}\n",
    "error_list = []\n",
    "user_temperature = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0280a471-ed31-46c6-9410-62896ed1f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_options_formatted = []\n",
    "for i in data['choices']:\n",
    "    input_cleaned = re.sub(r\"array\\(\\s*\\[(.*?)\\]\\s*,\\s*dtype=object\\)\", r\"[\\1]\", i, flags=re.DOTALL)\n",
    "    try:\n",
    "        extracted_dict = eval(input_cleaned, safe_globals)\n",
    "        labels = extracted_dict.get('label', [])\n",
    "        texts = extracted_dict.get('text', [])\n",
    "        if len(labels) == len(texts):\n",
    "            formatted_output = '\\n'.join([f\"{label}: {text}\" for label, text in zip(labels, texts)])\n",
    "            answer_options_formatted.append(formatted_output)\n",
    "        else:\n",
    "            print(f\"Skipping row due to mismatched lengths or missing data: {input_cleaned}\")\n",
    "    \n",
    "    except (SyntaxError, NameError, TypeError) as e:\n",
    "        print(f\"Error parsing row: {input_cleaned}\\nError details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3a048e-445b-4f37-b0f4-ae4721bf4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['answer_options_formatted'] = answer_options_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cefe08d0-f20c-4a28-9bd6-dc5059ec1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['answer'] = data['answerKey'].replace({'A': 0, 'B': 1, 'C': 2, 'D': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72baecbd-5c55-4ed7-a9da-21b758911e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_list = list()\n",
    "for i in data['answer_options_formatted']:\n",
    "    output_list = [line.split(': ', 1)[1] for line in i.splitlines()]\n",
    "    choice_list.append(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07ac34c-4873-4769-a8a3-f7112cc21dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['choices'] = choice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d11614d-7bb3-455a-992f-55efbbd50fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = data['choices'].tolist()\n",
    "data['choices_copy'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2353551-ed92-4c12-90d6-dcb951ba81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_choice(choice_list, correct_index):\n",
    "    choice_list_copy = choice_list[:]\n",
    "    masked_options = []\n",
    "    incorrect_indices = [i for i in range(len(choice_list_copy)) if i != correct_index]\n",
    "    \n",
    "    if incorrect_indices:\n",
    "        mask_index_1 = random.choice(incorrect_indices)\n",
    "        masked_options.append(choice_list_copy[mask_index_1])  \n",
    "        choice_list_copy[mask_index_1] = \"\"  \n",
    "        incorrect_indices.remove(mask_index_1)  \n",
    "    \n",
    "    return choice_list_copy, masked_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85100682-2dfc-452b-b509-a70073c512a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['choices_copy', 'masked_options']] = data.apply(\n",
    "    lambda row: pd.Series(mask_choice(row['choices_copy'], row['answer'])), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac952982-71a9-4cc8-9e2e-bcef4f1e9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question_answer(row):\n",
    "    question = f\"Question: {row['question']}\\nAnswer:\\n\"\n",
    "    \n",
    "    choices = row['choices_copy']\n",
    "    options_text = [\n",
    "        f\"A: {str(choices[0]) if len(choices) > 0 else ''}\",\n",
    "        f\"B: {str(choices[1]) if len(choices) > 1 else ''}\",\n",
    "        f\"C: {str(choices[2]) if len(choices) > 2 else ''}\",\n",
    "        f\"D: {str(choices[3]) if len(choices) > 3 else ''}\",\n",
    "        f\"E: {str(choices[4]) if len(choices) > 4 else ''}\"\n",
    "    ]\n",
    "    answer_options = \"\\n\".join(options_text)\n",
    "    \n",
    "    return question + answer_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3516f847-117e-419b-a87f-c5725a35c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question_answer'] = data.apply(format_question_answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b120e12-e9b2-44dc-8046-d89a28588d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bc5ddb117e462da650dbedc3c0e512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56fb64c5-a671-4622-aff7-ac51f9bf9f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for index, row in data[:].iterrows():\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Q:Complete the missing option in one word: [\n",
    "        Q: Which of the following countries generated the most total energy from solar sources in 2019?\n",
    "        Answer:\n",
    "        A: China\n",
    "        B: United States\n",
    "        C: \n",
    "        D: Japan\n",
    "        ]. \n",
    "        Reply with ONLY ONE word the missing option which is enclosed in **[ANSWER]**, not with the entire sentence.\n",
    "        A: **Germany**\n",
    "        Similarly, complete the missing option in one word: [{row['question_answer']}]. Reply with ONLY ONE word which is enclosed in **[ANSWER]**, not with the entire sentence. Only answer the option asked, do not reply with any other information. Do not assume anything else is given to you other than the question and the options. Take the question and options given, and help deduce the missing option.\"\"\"\n",
    "    response = pipeline(prompt, max_new_tokens=10, temperature=0.5, top_p=0.9)\n",
    "    response_text = response[0]['generated_text'][len(prompt):].strip()\n",
    "    # answer = generated_text[len(prompt):].strip()\n",
    "    result[row['question']] = response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8772e8b1-aaaa-41fb-8ab0-7fef96f8f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_final_answer(response_text):\n",
    "    match = re.search(r\"\\*\\*\\s*\\[?([^*\\[\\]]+)\\]?\\s*\\*\\*\", response_text)\n",
    "    if match:\n",
    "        candidate = match.group(1).strip()\n",
    "        if candidate and candidate != \"[ANSWER]\":  \n",
    "            return candidate\n",
    "    second_answer_match = re.search(r\"A:\\s*\\[?([^\\[\\]]+)\\]?\", response_text)\n",
    "    if second_answer_match:\n",
    "        candidate = second_answer_match.group(1).strip()\n",
    "        if candidate and candidate != \"[ANSWER]\":\n",
    "            return candidate\n",
    "    return \"Invalid response\"\n",
    "\n",
    "processed_output = dict()\n",
    "\n",
    "for key, val in result.items():\n",
    "    processed_output[key] = extract_final_answer(val).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c85c0300-a2d3-40ae-b368-258edcce51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for index, row in data.iterrows():\n",
    "    if row['masked_options'] and len(row['masked_options']) > 0:\n",
    "        first_option = row['masked_options'][0].lower()\n",
    "        comparison = processed_output[row['question']].lower()\n",
    "        if first_option == comparison:\n",
    "            acc += 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f54fdba-cdec-439a-a00d-279b2e8b4218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Llama-3.2-3B-Instruct to predict the missing option is: 1.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The accuracy of Llama-3.2-3B-Instruct to predict the missing option is: {round(acc*100.0/len(result),2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
